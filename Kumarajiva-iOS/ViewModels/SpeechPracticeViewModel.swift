import Foundation
import SwiftUI
import AVFoundation

@MainActor
class SpeechPracticeViewModel: NSObject, ObservableObject {
    @Published var records: [SpeechPracticeRecord] = []
    @Published var currentScore: Int = 0
    @Published var isRecording = false
    @Published var recordingTime: TimeInterval = 0
    @Published var recognizedText = ""
    @Published var wordResults: [WordMatchResult] = []
    
    // æ·»åŠ å®æ—¶è¯†åˆ«ç›¸å…³å±æ€§
    @Published var isTranscribing = false
    @Published var interimResult = ""
    @Published var transcriptionProgress: Float = 0.0
    
    // æ·»åŠ å­¦ä¹ è®°å½•ç›¸å…³å±æ€§
    @Published var studyRecords: [StudyRecord] = []
    @Published var isLoadingStudyRecords = false
    @Published var studyRecordsError: String?
    
    // æ·»åŠ æ™ºèƒ½è§£æç›¸å…³å±æ€§
    @Published var analysisState: AnalysisState = .notAnalyzed
    @Published var isAnalyzing = false
    
    private let speechService = SpeechRecognitionService.shared
    private var audioPlayer: AVAudioPlayer?
    private var playbackCompletionHandler: (() -> Void)?
    
    override init() {
        super.init()
        // ä»æœåŠ¡è·å–è®°å½•
        records = SpeechPracticeRecordService.shared.records
        
        // Bind to speech service properties
        speechService.$isRecording.assign(to: &$isRecording)
        speechService.$recordingTime.assign(to: &$recordingTime)
        speechService.$recognizedText.assign(to: &$recognizedText)
        speechService.$wordResults.assign(to: &$wordResults)
        
        // ç»‘å®šæ–°çš„å®æ—¶è¯†åˆ«å±æ€§
        if let whisperService = speechService as? WhisperKitService {
            whisperService.$isTranscribing.assign(to: &$isTranscribing)
            whisperService.$interimResult.assign(to: &$interimResult)
            whisperService.$transcriptionProgress.assign(to: &$transcriptionProgress)
        } else {
            // WhisperKitService.shared is not optional, so we don't need if let
            let whisperService = WhisperKitService.shared
            whisperService.$isTranscribing.assign(to: &$isTranscribing)
            whisperService.$interimResult.assign(to: &$interimResult)
            whisperService.$transcriptionProgress.assign(to: &$transcriptionProgress)
        }
    }
    
    // Start recording
    func startRecording() {
        // Stop any audio playback first
        AudioService.shared.stopPlayback()
        
        if audioPlayer != nil {
            audioPlayer?.stop()
            audioPlayer = nil
        }
        
        // Reset state for new recording
        recognizedText = ""
        wordResults = []
        
        // Ensure audio session is properly reset before starting new recording
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { [weak self] in
            self?.speechService.startRecording()
        }
    }
    
    // Stop recording and save the record
    func stopRecording(word: String, example: String, shouldSave: Bool = true) {
        print("SpeechPracticeViewModel: åœæ­¢å½•éŸ³, å•è¯: \(word), ä¿å­˜: \(shouldSave)")
        
        // Use a background queue for the blocking semaphore calls
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }
            
            print("SpeechPracticeViewModel: å¼€å§‹åœæ­¢å½•éŸ³å¤„ç†")
            guard let audioURL = self.speechService.stopRecording() else { 
                print("SpeechPracticeViewModel: åœæ­¢å½•éŸ³å¤±è´¥ï¼Œæœªè·å–åˆ°éŸ³é¢‘URL")
                DispatchQueue.main.async {
                    self.recognizedText = ""
                    self.wordResults = []
                    self.currentScore = 0
                }
                return 
            }
            
            print("SpeechPracticeViewModel: å½•éŸ³å·²åœæ­¢ï¼ŒéŸ³é¢‘URL: \(audioURL)")
            
            // å¦‚æœé€‰æ‹©æ”¾å¼ƒï¼Œåˆ™ä¸ä¿å­˜è®°å½•
            if !shouldSave {
                print("SpeechPracticeViewModel: ç”¨æˆ·é€‰æ‹©ä¸ä¿å­˜å½•éŸ³")
                // åˆ é™¤ä¸´æ—¶å½•éŸ³æ–‡ä»¶
                do {
                    try FileManager.default.removeItem(at: audioURL)
                    print("SpeechPracticeViewModel: ä¸´æ—¶å½•éŸ³æ–‡ä»¶å·²åˆ é™¤")
                } catch {
                    print("Failed to delete canceled recording file: \(error)")
                }
                
                // é‡ç½®çŠ¶æ€
                DispatchQueue.main.async {
                    self.recognizedText = ""
                    self.wordResults = []
                    self.currentScore = 0
                }
                return
            }
            
            // ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç¡®ä¿WhisperKitæœ‰è¶³å¤Ÿæ—¶é—´å¤„ç†å½•éŸ³
            if UserSettings.shared.speechRecognitionServiceType == .whisperKit {
                print("SpeechPracticeViewModel: ä½¿ç”¨WhisperKitï¼Œç­‰å¾…å¤„ç†å®Œæˆ")
                Thread.sleep(forTimeInterval: 1.0) // ç­‰å¾…1ç§’ï¼Œç¡®ä¿å¤„ç†å®Œæˆ
            }
            
            // æ­£å¸¸ä¿å­˜è®°å½•çš„é€»è¾‘
            // Calculate score based on the example text
            print("SpeechPracticeViewModel: è®¡ç®—å¾—åˆ†ï¼Œä¾‹å¥: \(example)")
            let score = self.speechService.calculateScore(expectedText: example)
            print("SpeechPracticeViewModel: å¾—åˆ†è®¡ç®—ç»“æœ: \(score)")
            
            // å¦‚æœä½¿ç”¨WhisperKitï¼Œç¡®ä¿æˆ‘ä»¬æœ‰è¯†åˆ«ç»“æœ
            if UserSettings.shared.speechRecognitionServiceType == .whisperKit {
                print("SpeechPracticeViewModel: WhisperKitè¯†åˆ«ç»“æœ: \(WhisperKitService.shared.recognizedText)")
                // æ‰‹åŠ¨åŒæ­¥è¯†åˆ«ç»“æœ
                DispatchQueue.main.async {
                    self.recognizedText = WhisperKitService.shared.recognizedText
                    self.wordResults = WhisperKitService.shared.wordResults
                }
            }
            
            // Update UI on main thread
            DispatchQueue.main.async {
                self.currentScore = score
                
                print("SpeechPracticeViewModel: åˆ›å»ºå½•éŸ³è®°å½•ï¼Œå¾—åˆ†: \(score)")
                // Create and save the record
                let record = SpeechPracticeRecord(
                    word: word,
                    example: example,
                    audioURL: audioURL,
                    timestamp: Date(),
                    score: score
                )
                
                // ä½¿ç”¨æœåŠ¡æ·»åŠ è®°å½•
                SpeechPracticeRecordService.shared.addRecord(record)
                // æ›´æ–°æœ¬åœ°è®°å½•æ•°ç»„
                self.records = SpeechPracticeRecordService.shared.records
                print("SpeechPracticeViewModel: å½•éŸ³è®°å½•å·²ä¿å­˜")
            }
        }
    }
    
    // å–æ¶ˆå½•éŸ³å¹¶æ¸…é™¤çŠ¶æ€
    func cancelRecording() {
        // Use a background queue for the blocking call
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }
            
            _ = self.speechService.stopRecording()
            
            // é‡ç½®çŠ¶æ€ - on main thread
            DispatchQueue.main.async {
                self.recognizedText = ""
                self.wordResults = []
                self.currentScore = 0
            }
        }
    }
    
    // Play a recording
    func playRecording(at url: URL, completion: (() -> Void)? = nil) {
        do {
            // Stop any other audio playing
            audioPlayer?.stop()
            AudioService.shared.stopPlayback()
            
            // Save completion handler
            playbackCompletionHandler = completion
            
            // Configure audio session for playback
            try AVAudioSession.sharedInstance().setCategory(.playback)
            try AVAudioSession.sharedInstance().setActive(true)
            
            // Create and play the audio player
            audioPlayer = try AVAudioPlayer(contentsOf: url)
            audioPlayer?.delegate = self
            audioPlayer?.prepareToPlay()
            audioPlayer?.play()
        } catch {
            print("Failed to play recording: \(error)")
            completion?()
        }
    }
    
    // Stop playback
    func stopPlayback() {
        audioPlayer?.stop()
        audioPlayer = nil
        playbackCompletionHandler = nil
    }
    
    // Delete a recording
    func deleteRecord(id: UUID) {
        // Stop if currently playing this recording
        if audioPlayer != nil {
            audioPlayer?.stop()
            audioPlayer = nil
            playbackCompletionHandler = nil
        }
        
        // ä½¿ç”¨æœåŠ¡åˆ é™¤è®°å½•
        SpeechPracticeRecordService.shared.deleteRecord(id: id)
        // æ›´æ–°æœ¬åœ°è®°å½•æ•°ç»„
        records = SpeechPracticeRecordService.shared.records
    }
    
    // Delete all recordings for a specific word
    func deleteAllRecordsForWord(_ word: String) {
        // Stop any playback
        if audioPlayer != nil {
            audioPlayer?.stop()
            audioPlayer = nil
            playbackCompletionHandler = nil
        }
        
        // ä½¿ç”¨æœåŠ¡åˆ é™¤è®°å½•
        SpeechPracticeRecordService.shared.deleteAllRecordsForWord(word)
        // æ›´æ–°æœ¬åœ°è®°å½•æ•°ç»„
        records = SpeechPracticeRecordService.shared.records
        print("å·²åˆ é™¤å…³äº '\(word)' çš„ç»ƒä¹ è®°å½•")
    }
    
    // è¿™äº›æ–¹æ³•å·²ç”± SpeechPracticeRecordService å¤„ç†
    // ä¿ç•™æ­¤æ³¨é‡Šä»¥è¡¨æ˜è¿™äº›åŠŸèƒ½å·²ç§»è‡³æœåŠ¡ä¸­
    
    // Format recording time as MM:SS
    func formatRecordingTime(_ time: TimeInterval) -> String {
        let minutes = Int(time) / 60
        let seconds = Int(time) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
    
    // Get color for word match type
    func colorForMatchType(_ type: WordMatchResult.MatchType) -> Color {
        switch type {
        case .matched:
            return .green      // åŒ¹é…çš„è¯ - ç»¿è‰²
        case .missing:
            return .red        // ç¼ºå¤±çš„è¯ - çº¢è‰²
        case .extra:
            return .gray       // å¤šä½™çš„è¯ - ç°è‰²
        }
    }
    
    // Format recognized text with highlights
    func formattedRecognizedText() -> [WordMatchResult] {
        // No highlighting if no words
        if wordResults.isEmpty {
            if recognizedText.isEmpty {
                return []
            } else {
                // Return existing text without highlighting
                return recognizedText.components(separatedBy: .whitespaces)
                    .filter { !$0.isEmpty }
                    .enumerated()
                    .map { index, word in 
                        WordMatchResult(word: word, type: .extra, originalIndex: index) 
                    }
            }
        }
        
        return wordResults
    }
    
    // MARK: - Study Records Methods
    
    // è·å–å•è¯å­¦ä¹ è®°å½•
    func fetchStudyRecords(for word: String) {
        isLoadingStudyRecords = true
        studyRecordsError = nil
        
        Task {
            do {
                let records = try await APIService.shared.getWordStudyHistory(word: word)
                DispatchQueue.main.async {
                    self.studyRecords = records
                    self.isLoadingStudyRecords = false
                }
            } catch {
                DispatchQueue.main.async {
                    self.studyRecordsError = error.localizedDescription
                    self.isLoadingStudyRecords = false
                    self.studyRecords = []
                }
                print("è·å–å­¦ä¹ è®°å½•å¤±è´¥: \(error)")
            }
        }
    }
    
    // æ¸…é™¤å­¦ä¹ è®°å½•
    func clearStudyRecords() {
        studyRecords = []
        studyRecordsError = nil
    }
    
    // MARK: - Word Analysis Methods
    
    /// è·å–å•è¯æ™ºèƒ½è§£æ
    func fetchWordAnalysis(for word: String, forceRefresh: Bool = false) {
        // æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ä¸”ä¸å¼ºåˆ¶åˆ·æ–°
        if !forceRefresh, let cachedAnalysis = WordAnalysisService.shared.getAnalysis(for: word) {
            analysisState = .analyzed(cachedAnalysis)
            return
        }
        
        analysisState = .analyzing
        isAnalyzing = true
        
        Task {
            do {
                let prompt = generateAnalysisPrompt(for: word)
                let response = try await LLMService.shared.sendChatMessage(prompt: prompt)
                
                // è§£æLLMå“åº”
                if let analysis = parseAnalysisResponse(response, word: word) {
                    // ä¿å­˜åˆ°ç¼“å­˜
                    WordAnalysisService.shared.saveAnalysis(analysis)
                    
                    DispatchQueue.main.async {
                        self.analysisState = .analyzed(analysis)
                        self.isAnalyzing = false
                    }
                } else {
                    DispatchQueue.main.async {
                        self.analysisState = .failed("è§£æç»“æœæ ¼å¼é”™è¯¯")
                        self.isAnalyzing = false
                    }
                }
            } catch {
                DispatchQueue.main.async {
                    self.analysisState = .failed(error.localizedDescription)
                    self.isAnalyzing = false
                }
                print("ğŸ§  [Analysis] æ™ºèƒ½è§£æå¤±è´¥: \(error)")
            }
        }
    }
    
    /// ç”Ÿæˆåˆ†ææç¤ºè¯
    private func generateAnalysisPrompt(for word: String) -> String {
        return """
        æ ¹æ®äººç±»è®°å¿†åŸç†å’Œäººæ€§ï¼Œæˆ‘ç«‹é©¬è®°ä½è¿™ä¸ªå•è¯ä¸”ç»ˆèº«éš¾å¿˜ã€‚
        1ã€ä¸¾ä¾‹ï¼šubiquitous
        2ã€æŒ‰ç…§ Json æ ¼å¼è¾“å‡ºï¼š
        {
            "word": "ubiquitous",
            "basic_info": {
                "phonetic_notation": {
                    "British": "/juËËˆbÉªkwÉªtÉ™s/",
                    "American": "/juËËˆbÉªkwÉªtÉ™s/"
                },
                "annotation": "adj. æ™®éå­˜åœ¨çš„ï¼›æ— å¤„ä¸åœ¨çš„"
            },
            "split_association_method": "æŠŠ"ubiquitous"æ‹†åˆ†æˆ"uï¼ˆçœ‹ä½œ'you'ï¼Œä½ ï¼‰ + biï¼ˆè°éŸ³'å¿…'ï¼‰ + quitï¼ˆç¦»å¼€ï¼‰ + ousï¼ˆå½¢å®¹è¯åç¼€ï¼‰" ã€‚è”æƒ³æˆ"ä½ å¿…ç¦»å¼€"ä¸€ä¸ªåœ°æ–¹ï¼Œä½†æ— è®ºä½ èµ°åˆ°å“ªå„¿ï¼Œéƒ½èƒ½å‘ç°æŸä¸ªäº‹ç‰©ï¼Œè¿™å°±è¯´æ˜è¿™ä¸ªäº‹ç‰©æ˜¯"æ™®éå­˜åœ¨çš„ï¼›æ— å¤„ä¸åœ¨çš„" ã€‚",
            "scene_memory": [{
                    "scene": "å¦‚ä»Šï¼Œæ™ºèƒ½æ‰‹æœºå‡ ä¹æ˜¯"ubiquitous"çš„ã€‚åœ¨å…¬äº¤ä¸Šã€é¤å…é‡Œã€æ ¡å›­ä¸­ï¼Œéšå¤„éƒ½èƒ½çœ‹åˆ°äººä»¬æ‹¿ç€æ™ºèƒ½æ‰‹æœºï¼Œå®ƒå·²ç»æˆä¸ºäººä»¬ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œæ— å¤„ä¸åœ¨ã€‚"
                },
                {
                    "scene": "åœ¨ç°ä»£åŸå¸‚ä¸­ï¼ŒWiFiä¿¡å·å‡ ä¹æ˜¯ubiquitousçš„ï¼Œæ— è®ºèµ°åˆ°å“ªé‡Œéƒ½èƒ½è¿æ¥ç½‘ç»œã€‚"
                }
            ],
            "synonym_precise_guidance": [{
                    "synonym": "Universal",
                    "explanation": "å¼ºè°ƒåœ¨æ‰€æœ‰åœ°æ–¹æˆ–æ‰€æœ‰äººä¸­éƒ½å­˜åœ¨ï¼Œæ›´å…·æ™®éæ€§ï¼Œå¸¸æ¶‰åŠæ¦‚å¿µã€çœŸç†ã€ç°è±¡ç­‰ï¼Œå¦‚universal truthï¼ˆæ™®éçœŸç†ï¼‰ã€‚"
                },
                {
                    "synonym": "Widespread",
                    "explanation": "å¼ºè°ƒåˆ†å¸ƒå¹¿æ³›ï¼Œä½†ä¸ä¸€å®šåœ¨æ¯ä¸ªåœ°æ–¹éƒ½å­˜åœ¨ï¼Œå¦‚widespread supportï¼ˆå¹¿æ³›æ”¯æŒï¼‰ã€‚"
                }
            ]
        }

        æ–°å•è¯ï¼š\(word)
        """
    }
    
    /// è§£æLLMå“åº”ä¸ºWordAnalysiså¯¹è±¡
    private func parseAnalysisResponse(_ response: String, word: String) -> WordAnalysis? {
        // æå–JSONéƒ¨åˆ†
        guard let jsonData = extractJSONFromResponse(response) else {
            print("ğŸ§  [Analysis] æ— æ³•ä»å“åº”ä¸­æå–JSON")
            return nil
        }
        
        do {
            let llmResponse = try JSONDecoder().decode(LLMAnalysisResponse.self, from: jsonData)
            
            // è½¬æ¢ä¸ºWordAnalysiså¯¹è±¡
            let analysis = WordAnalysis(
                word: word,
                basicInfo: llmResponse.basicInfo,
                splitAssociationMethod: llmResponse.splitAssociationMethod,
                sceneMemory: llmResponse.sceneMemory,
                synonymPreciseGuidance: llmResponse.synonymPreciseGuidance,
                createdAt: Date(),
                updatedAt: Date()
            )
            
            return analysis
        } catch {
            print("ğŸ§  [Analysis] JSONè§£æå¤±è´¥: \(error)")
            return nil
        }
    }
    
    /// ä»LLMå“åº”ä¸­æå–JSONéƒ¨åˆ†
    private func extractJSONFromResponse(_ response: String) -> Data? {
        // æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        guard let firstBrace = response.firstIndex(of: "{"),
              let lastBrace = response.lastIndex(of: "}") else {
            return nil
        }
        
        let jsonString = String(response[firstBrace...lastBrace])
        return jsonString.data(using: .utf8)
    }
    
    /// æ¸…ç©ºåˆ†æçŠ¶æ€
    func clearAnalysisState() {
        analysisState = .notAnalyzed
        isAnalyzing = false
    }
}

// MARK: - AVAudioPlayerDelegate
extension SpeechPracticeViewModel: @preconcurrency AVAudioPlayerDelegate {
    nonisolated func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            
            // Call completion handler and clean up
            self.playbackCompletionHandler?()
            self.playbackCompletionHandler = nil
            self.audioPlayer = nil
        }
    }
}